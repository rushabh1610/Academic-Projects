{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rushabh vakharia\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "c:\\users\\rushabh vakharia\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import requests      \n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup  \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "import re, csv, string \n",
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import dateutil.parser\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defined functions to scrape Bitcoin news from news.bitcoin.com and bloomberg.com and to fetch headlines and date to be used later in sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate headlines out from 'all_data_raw' variable so that it can be written to separate columns in csv\n",
    "\n",
    "def get_headlines(data, index):\n",
    "    \n",
    "    headlines = []\n",
    "    \n",
    "    for row in data:\n",
    "        headlines.append(row[index])\n",
    "        \n",
    "    return headlines\n",
    "\n",
    "#Separate dates out from 'all_data' variable so that it can be written to separate columns in csv\n",
    "\n",
    "def get_date(data, index):\n",
    "    \n",
    "    date = []\n",
    "    \n",
    "    for row in data:\n",
    "        date.append(row[index])\n",
    "        \n",
    "    return date\n",
    "\n",
    "#Handle cleaning of one specific date format\n",
    "\n",
    "def tokenize_date(t):\n",
    "    \n",
    "    pattern=r'[a-zA-Z{3}]+[.\\s]+[\\d{1,2}\\,\\s]+[\\d{4}]+'   \n",
    "\n",
    "    date_pattern = nltk.regexp_tokenize(t, pattern)\n",
    "    \n",
    "    return date_pattern\n",
    "\n",
    "#Get Bitcoin news from news.bitcoin.com\n",
    "\n",
    "def get_bitcoinNews():\n",
    "    \n",
    "    headlines=[]  #List to store headlines\n",
    "    dates = []    #List to store date\n",
    "    raw_headlines = []\n",
    "    page_number = 1\n",
    "    page_url=\"https://news.bitcoin.com/page/\"+str(page_number)+\"/?s=bitcoin\"\n",
    "    \n",
    "    while page_url!=\"https://news.bitcoin.com/page/140/?s=bitcoin\":\n",
    "        \n",
    "        if page_number % 20 == 0:\n",
    "            print('Scraped %d of 140 pages' % page_number)\n",
    "            \n",
    "        page_url=\"https://news.bitcoin.com/page/\"+str(page_number)+\"/?s=bitcoin\"        \n",
    "        page = requests.get(page_url) \n",
    "        page_number += 1\n",
    "        \n",
    "        if page.status_code!=200:  \n",
    "            page_number = None\n",
    "        else:                   \n",
    "            soup = BeautifulSoup(page.content, 'html.parser')                        \n",
    "            \n",
    "            main_content = soup.find('div', class_ = 'td-ss-main-content')            \n",
    "            h3s = main_content.find_all('h3', class_ = \"entry-title td-module-title\")\n",
    "            span_dates = main_content.find_all(\"span\", class_ = \"td-post-date\")\n",
    "            \n",
    "            for idx, h3 in enumerate(h3s):\n",
    "                header = h3.select('a')\n",
    "                \n",
    "                if header != []:\n",
    "                    headline = header[0].get_text().lower()\n",
    "                    raw_headlines.append(headline)\n",
    "            \n",
    "            for idx, span in enumerate(span_dates):\n",
    "                dates_list = span.select('time')\n",
    "                \n",
    "                if dates_list != []:\n",
    "                    date = dates_list[0].get_text()\n",
    "                    dates.append(date)\n",
    "    \n",
    "    raw_data = zip(raw_headlines, dates)\n",
    "    return raw_data\n",
    "\n",
    "#Get Bitcoin news from bloomberg.com\n",
    "\n",
    "def get_bloomberg():\n",
    "    \n",
    "    headlines=[]  #List to store headlines\n",
    "    dates = []    #List to store date\n",
    "    raw_headlines = [] \n",
    "    page_number = 1\n",
    "    page_url=\"https://www.bloomberg.com/search?query=bitcoin&sort=time:desc&endTime=2017-11-29T17:35:17.135Z&page=\"+str(page_number)\n",
    "    \n",
    "    while page_url!=\"https://www.bloomberg.com/search?query=bitcoin&sort=time:desc&endTime=2017-11-29T17:35:17.135Z&page=80\":     \n",
    "\n",
    "        if page_number % 20 == 0:\n",
    "            print('Scraped %d of 80 pages' % page_number)\n",
    "            \n",
    "        page_url=\"https://www.bloomberg.com/search?query=bitcoin&sort=time:desc&endTime=2017-11-29T17:35:17.135Z&page=\"+str(page_number)\n",
    "        page = requests.get(page_url) \n",
    "        page_number += 1\n",
    "        \n",
    "        if page.status_code!=200:  \n",
    "            page_number = 80\n",
    "        else:                   \n",
    "            soup = BeautifulSoup(page.content, 'html.parser')                        \n",
    "            \n",
    "            for header in soup.find_all('h1', class_ ='search-result-story__headline'):\n",
    "                \n",
    "                headline = header.get_text().lower()\n",
    "                raw_headlines.append(headline)\n",
    "                \n",
    "            for date in soup.find_all('time', class_ = 'published-at'):\n",
    "                \n",
    "                date_published = date.get_text()\n",
    "                date_published = date_published.lstrip()\n",
    "                dates.append(date_published)\n",
    "                 \n",
    "    raw_data = zip(raw_headlines, dates)\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling the functions defined above to scrape the data and then store it in the list \"all_data_raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping bitcoin news from news.bitcoin.com\n",
      "*******************************************\n",
      "\n",
      "Scraped 20 of 140 pages\n",
      "Scraped 40 of 140 pages\n",
      "Scraped 60 of 140 pages\n",
      "Scraped 80 of 140 pages\n",
      "Scraped 100 of 140 pages\n",
      "Scraped 120 of 140 pages\n",
      "Scraped 140 of 140 pages\n",
      "\n",
      "Scraping bitcoin news from bloomberg.com\n",
      "*******************************************\n",
      "\n",
      "Scraped 20 of 80 pages\n",
      "Scraped 40 of 80 pages\n",
      "Scraped 60 of 80 pages\n",
      "Scraped 80 of 80 pages\n",
      "\n",
      "Done scraping all data and stored in list 'all_data_raw'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"Scraping bitcoin news from news.bitcoin.com\")\n",
    "    print(\"*******************************************\\n\")\n",
    "    bitcoinNews_raw = get_bitcoinNews()\n",
    "    \n",
    "    print(\"\\nScraping bitcoin news from bloomberg.com\")\n",
    "    print(\"*******************************************\\n\")\n",
    "    bloomberg_raw = get_bloomberg()\n",
    "    \n",
    "    all_data_raw = []\n",
    "    \n",
    "    #Appending the headlines and date to the list\n",
    "    \n",
    "    all_data_raw.extend(list(bitcoinNews_raw))\n",
    "    all_data_raw.extend(list(bloomberg_raw))\n",
    "    \n",
    "    print(\"\\nDone scraping all data and stored in list 'all_data_raw'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying the headlines as positive, negative and neutral and printing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral headlines:  1166\n",
      "Somewhat negative headlines:  252\n",
      "Very negative headlines:  50\n",
      "Somewhat positive headlines:  328\n",
      "Very positive headlines:  52\n",
      "Total number of headlines:  1848\n"
     ]
    }
   ],
   "source": [
    "#Filter out headlines without these keywords\n",
    "\n",
    "keywords = ['bitcoin', 'cryptocurrency','cryptocurrencies', 'crypto', 'blockchain', 'blockchains']\n",
    "\n",
    "#Get headlines and dates as individual lists\n",
    "\n",
    "headlines = get_headlines(list(filter(lambda x: any(word in x[0] for word in keywords), all_data_raw)),0)\n",
    "dates = get_date(list(filter(lambda x: any(word in x[0] for word in keywords), all_data_raw)),1)\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "compound = []\n",
    "\n",
    "for head in headlines:\n",
    "    \n",
    "    head_lower = head.lower()\n",
    "    ss = sid.polarity_scores(head_lower)\n",
    "    compound.append(ss['compound'])\n",
    "\n",
    "#Counting the number of headlines in each sentiment class\n",
    "\n",
    "neutral = []\n",
    "somewhat_negative = []\n",
    "somewhat_positive = []\n",
    "very_negative = []\n",
    "very_positive = []\n",
    "\n",
    "for index, score in enumerate(compound):\n",
    "    \n",
    "    if score > -0.20 and score < 0.20:\n",
    "        neutral.append(score)\n",
    "        \n",
    "    elif score > -0.60 and score < -0.20:\n",
    "        somewhat_negative.append(score)\n",
    "        \n",
    "    elif score > 0.20 and score < 0.60:   \n",
    "        somewhat_positive.append(score)\n",
    "        \n",
    "    elif score <= -0.60:\n",
    "        very_negative.append(score)\n",
    "        \n",
    "    else:\n",
    "        very_positive.append(score)\n",
    "\n",
    "print('Neutral headlines: ', len(neutral))\n",
    "print('Somewhat negative headlines: ', len(somewhat_negative))\n",
    "print(\"Very negative headlines: \", len(very_negative))\n",
    "print('Somewhat positive headlines: ', len(somewhat_positive))\n",
    "print(\"Very positive headlines: \", len(very_positive))\n",
    "print('Total number of headlines: ', len(compound))\n",
    "\n",
    "data_with_sentiment = list(zip(dates, headlines, compound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing, lemmatizing and removing frequent keywords to improve clustering performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize and lemmatize headlines\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    \n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    \n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    \n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    \n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def lemmatize(document):\n",
    "    \n",
    "    pattern=r'[a-zA-Z]+[a-zA-Z\\-]+[a-zA-Z]'      \n",
    "    tokens=nltk.regexp_tokenize(document, pattern)\n",
    "    tagged_tokens=nltk.pos_tag(tokens)\n",
    "    wordnet_lemmatizer=WordNetLemmatizer()\n",
    "    \n",
    "    le_words=[wordnet_lemmatizer.lemmatize(word, get_wordnet_pos(tag)) \\\n",
    "              for (word, tag) in tagged_tokens \\\n",
    "              if word not in stop_words and \\\n",
    "              word not in string.punctuation]\n",
    "    \n",
    "    le_words = list(set(le_words))\n",
    "    return le_words\n",
    "\n",
    "lem_headlines = []\n",
    "\n",
    "for headline in headlines:\n",
    "    \n",
    "    lem_headline = lemmatize(headline)\n",
    "    lem_headlines.append(lem_headline)\n",
    "\n",
    "lem_data_with_sentiment = list(zip(dates, headlines, lem_headlines, compound))\n",
    "\n",
    "#Function to remove frequent keywords from lemmatized headline tokens to improve clustering performance\n",
    "\n",
    "def remove_keywords(data):\n",
    "    for row in data:\n",
    "        lem_list = row[2]\n",
    "        for word in lem_list:\n",
    "            if word in keywords:\n",
    "                lem_list.remove(word)\n",
    "        \n",
    "    return data\n",
    "\n",
    "#Preparing data for LDA clustering\n",
    "\n",
    "data_for_clustering = remove_keywords(lem_data_with_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LDA clustering model for 5 clusters and storing the data in \"Cleaned_data_final_5clusters_btc.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.037*\"market\" + 0.018*\"price\" + 0.017*\"update\" + 0.010*\"new\" + 0.009*\"cash\"'), (1, '0.015*\"say\" + 0.011*\"cash\" + 0.010*\"bank\" + 0.010*\"big\" + 0.008*\"south\"'), (2, '0.034*\"cash\" + 0.015*\"new\" + 0.013*\"exchange\" + 0.009*\"gold\" + 0.008*\"network\"'), (3, '0.015*\"exchange\" + 0.014*\"launch\" + 0.012*\"fund\" + 0.011*\"future\" + 0.008*\"cash\"'), (4, '0.016*\"say\" + 0.010*\"fork\" + 0.010*\"bank\" + 0.009*\"may\" + 0.009*\"bubble\"')]\n",
      "\n",
      "All data cleaned, clustered and written to csv file 'Cleaned_data_final_5clusters_btc.csv'\n"
     ]
    }
   ],
   "source": [
    "#Building LDA Model for 5 clusters\n",
    "\n",
    "headline = get_headlines(data_for_clustering,2)\n",
    "dictionary = corpora.Dictionary(headline)\n",
    "corpus = [dictionary.doc2bow(row) for row in headline]\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word = dictionary, passes=20)\n",
    "print(ldamodel.print_topics(num_topics=5, num_words=5))\n",
    "\n",
    "#Classifying clusters and mapping them to headlines\n",
    "\n",
    "cluster = ldamodel[corpus]\n",
    "clusters = []\n",
    "list_ = []\n",
    "\n",
    "for t in cluster:\n",
    "    list_ = (list(zip(*t))[1])\n",
    "    clusters.append(list_.index(max(list_)))\n",
    "\n",
    "date, headline, tokenized_headline, sentiment = zip(*data_for_clustering)\n",
    "clean_data_5clusters = list(zip(date, headline, tokenized_headline, sentiment, clusters))\n",
    "\n",
    "#Displaying the headlines within each cluster\n",
    "\n",
    "filter(lambda x: x[1] == 0, clean_data_5clusters)\n",
    "filter(lambda x: x[1] == 1, clean_data_5clusters)\n",
    "filter(lambda x: x[1] == 2, clean_data_5clusters)\n",
    "filter(lambda x: x[1] == 3, clean_data_5clusters)\n",
    "filter(lambda x: x[1] == 4, clean_data_5clusters)\n",
    "\n",
    "#Writing cleaned data to csv for 5 clusters\n",
    "\n",
    "data_frame_5clusters = pd.DataFrame.from_records(clean_data_5clusters, columns = [\"Date\", \"Headline\", \"Tokenized Headline\", \"Sentiment\", \"Cluster\"])\n",
    "data_frame_5clusters.to_csv(\"Cleaned_data_final_5clusters_btc.csv\", encoding='utf-8')\n",
    "print(\"\\nAll data cleaned, clustered and written to csv file 'Cleaned_data_final_5clusters_btc.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LDA clustering model for 4 clusters and storing the data in \"Cleaned_data_final_4clusters_btc.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.026*\"market\" + 0.014*\"update\" + 0.013*\"price\" + 0.010*\"future\" + 0.007*\"million\"'), (1, '0.027*\"cash\" + 0.011*\"new\" + 0.009*\"fork\" + 0.009*\"gold\" + 0.008*\"bank\"'), (2, '0.021*\"cash\" + 0.013*\"exchange\" + 0.010*\"say\" + 0.006*\"fund\" + 0.006*\"bank\"'), (3, '0.014*\"exchange\" + 0.011*\"launch\" + 0.010*\"new\" + 0.007*\"first\" + 0.006*\"future\"')]\n",
      "\n",
      "All data cleaned, clustered and written to csv file 'Cleaned_data_final_4clusters_btc.csv'\n"
     ]
    }
   ],
   "source": [
    "#Building LDA Model for 4 clusters\n",
    " \n",
    "headline_list = get_headlines(data_for_clustering,2)\n",
    "dictionary = corpora.Dictionary(headline_list)\n",
    "corpus = [dictionary.doc2bow(row) for row in headline_list]\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=4, id2word = dictionary, passes=20)\n",
    "print(ldamodel.print_topics(num_topics=4, num_words=5))\n",
    "\n",
    "#Classifying clusters and mapping them to headlines\n",
    "\n",
    "cluster = ldamodel[corpus]\n",
    "clusters = []\n",
    "list_ = []\n",
    "\n",
    "for t in cluster:\n",
    "    list_ = (list(zip(*t))[1])\n",
    "    clusters.append(list_.index(max(list_)))\n",
    "\n",
    "date, headline, tokenized_headline, sentiment = zip(*data_for_clustering)\n",
    "clean_data_4clusters = list(zip(date, headline, tokenized_headline, sentiment, clusters))\n",
    "\n",
    "#Displaying the headlines within each cluster\n",
    "\n",
    "filter(lambda x: x[1] == 0, clean_data_4clusters)\n",
    "filter(lambda x: x[1] == 1, clean_data_4clusters)\n",
    "filter(lambda x: x[1] == 2, clean_data_4clusters)\n",
    "filter(lambda x: x[1] == 3, clean_data_4clusters)\n",
    "\n",
    "#Writing cleaned data to csv for 4 clusters\n",
    "\n",
    "data_frame_4clusters = pd.DataFrame.from_records(clean_data_4clusters, columns = [\"Date\", \"Headline\", \"Tokenized Headline\", \"Sentiment\", \"Cluster\"])\n",
    "data_frame_4clusters.to_csv(\"Cleaned_data_final_4clusters_btc.csv\", encoding='utf-8')\n",
    "print(\"\\nAll data cleaned, clustered and written to csv file 'Cleaned_data_final_4clusters_btc.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating pivot tables based on dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating pivot tables by date\n",
    "\n",
    "pivot_table_5 = pd.pivot_table(data_frame_5clusters, values = 'Sentiment', index = 'Date', columns = 'Cluster', aggfunc = np.sum, fill_value = 0)\n",
    "pivot_table_4 = pd.pivot_table(data_frame_4clusters, values = 'Sentiment', index = 'Date', columns = 'Cluster', aggfunc = np.sum, fill_value = 0)\n",
    "\n",
    "pivot_table_5 = pivot_table_5.rename_axis(None, axis =1).reset_index()\n",
    "pivot_table_4 = pivot_table_4.rename_axis(None, axis =1).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging pivot tables with bitcoin pricing data stored in files \"Bitcoin1Day.csv\" and \"Bitcoin3Day.csv\". These files have data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge pivot tables with bitcoin pricing data\n",
    "\n",
    "with open(\"Bitcoin1Day.csv\", \"r\") as f:\n",
    "    \n",
    "    reader=csv.reader(f, delimiter=',') \n",
    "    bitcoin_1day = [row for row in reader]\n",
    "    del bitcoin_1day[0]\n",
    "\n",
    "date_list = []\n",
    "\n",
    "for date in get_date(bitcoin_1day,0):\n",
    "    date = datetime.strptime(date, '%m/%d/%Y')\n",
    "    date = datetime.strftime(date,'%b %d, %Y')\n",
    "    date_list.append(date)\n",
    "    \n",
    "date, dailyreturn = zip(*bitcoin_1day)\n",
    "bitcoin_1day = zip(date_list,dailyreturn)\n",
    "    \n",
    "bitcoin_1day = pd.DataFrame.from_records(bitcoin_1day, columns = [\"Date\",\"DailyReturn\"])\n",
    "bitcoin_1day\n",
    "\n",
    "bitcoin_1day_5clusters = bitcoin_1day.merge(pivot_table_5, on = 'Date', how = 'left')\n",
    "bitcoin_1day_5clusters = bitcoin_1day_5clusters.dropna()\n",
    "\n",
    "bitcoin_1day_4clusters = bitcoin_1day.merge(pivot_table_4, on = 'Date', how = 'left')\n",
    "bitcoin_1day_4clusters = bitcoin_1day_4clusters.dropna()\n",
    "\n",
    "with open(\"Bitcoin3Day.csv\", \"r\") as f:\n",
    "    \n",
    "    reader=csv.reader(f, delimiter=',') \n",
    "    bitcoin_3day = [row for row in reader]\n",
    "    del bitcoin_3day[0]\n",
    "\n",
    "date_list = []\n",
    "\n",
    "for date in get_date(bitcoin_3day,0):\n",
    "    date = datetime.strptime(date, '%m/%d/%Y')\n",
    "    date = datetime.strftime(date,'%b %d, %Y')\n",
    "    date_list.append(date)\n",
    "    \n",
    "date, dailyreturn = zip(*bitcoin_3day)\n",
    "bitcoin_3day = zip(date_list,dailyreturn)\n",
    "    \n",
    "bitcoin_3day = pd.DataFrame.from_records(bitcoin_3day, columns = [\"Date\",\"DailyReturn\"])\n",
    "\n",
    "bitcoin_3day_5clusters = bitcoin_3day.merge(pivot_table_5, on = 'Date', how = 'left')\n",
    "bitcoin_3day_5clusters = bitcoin_3day_5clusters.dropna()\n",
    "\n",
    "bitcoin_3day_4clusters = bitcoin_3day.merge(pivot_table_4, on = 'Date', how = 'left')\n",
    "bitcoin_3day_4clusters = bitcoin_3day_4clusters.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            DailyReturn   R-squared:                       0.041\n",
      "Model:                            OLS   Adj. R-squared:                 -0.006\n",
      "Method:                 Least Squares   F-statistic:                    0.8697\n",
      "Date:                Fri, 27 Apr 2018   Prob (F-statistic):              0.504\n",
      "Time:                        00:00:26   Log-Likelihood:                -318.37\n",
      "No. Observations:                 107   AIC:                             648.7\n",
      "Df Residuals:                     101   BIC:                             664.8\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.1218      0.503      2.232      0.028       0.125       2.119\n",
      "0             -1.2802      2.011     -0.637      0.526      -5.269       2.708\n",
      "1              2.5094      1.947      1.289      0.200      -1.353       6.372\n",
      "2             -1.3691      1.642     -0.834      0.406      -4.626       1.888\n",
      "3             -1.7527      2.011     -0.871      0.386      -5.742       2.237\n",
      "4             -2.0655      2.255     -0.916      0.362      -6.539       2.408\n",
      "==============================================================================\n",
      "Omnibus:                        4.672   Durbin-Watson:                   2.010\n",
      "Prob(Omnibus):                  0.097   Jarque-Bera (JB):                6.059\n",
      "Skew:                          -0.102   Prob(JB):                       0.0483\n",
      "Kurtosis:                       4.148   Cond. No.                         5.02\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "******************************************************************************************************\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            DailyReturn   R-squared:                       0.046\n",
      "Model:                            OLS   Adj. R-squared:                  0.009\n",
      "Method:                 Least Squares   F-statistic:                     1.243\n",
      "Date:                Fri, 27 Apr 2018   Prob (F-statistic):              0.297\n",
      "Time:                        00:00:26   Log-Likelihood:                -318.08\n",
      "No. Observations:                 107   AIC:                             646.2\n",
      "Df Residuals:                     102   BIC:                             659.5\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.0059      0.472      2.132      0.035       0.070       1.942\n",
      "0              0.3574      1.874      0.191      0.849      -3.359       4.074\n",
      "1             -3.5128      1.718     -2.045      0.043      -6.920      -0.106\n",
      "2             -0.8055      1.501     -0.537      0.593      -3.782       2.171\n",
      "3              1.0804      1.643      0.658      0.512      -2.178       4.339\n",
      "==============================================================================\n",
      "Omnibus:                        6.375   Durbin-Watson:                   2.115\n",
      "Prob(Omnibus):                  0.041   Jarque-Bera (JB):                9.697\n",
      "Skew:                          -0.169   Prob(JB):                      0.00784\n",
      "Kurtosis:                       4.436   Cond. No.                         4.20\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "******************************************************************************************************\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            DailyReturn   R-squared:                       0.063\n",
      "Model:                            OLS   Adj. R-squared:                 -0.099\n",
      "Method:                 Least Squares   F-statistic:                    0.3879\n",
      "Date:                Fri, 27 Apr 2018   Prob (F-statistic):              0.853\n",
      "Time:                        00:00:26   Log-Likelihood:                -123.95\n",
      "No. Observations:                  35   AIC:                             259.9\n",
      "Df Residuals:                      29   BIC:                             269.2\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.7650      1.641      1.685      0.103      -0.591       6.121\n",
      "0              1.2254      7.577      0.162      0.873     -14.272      16.723\n",
      "1             -5.3032      6.240     -0.850      0.402     -18.064       7.458\n",
      "2             -4.0357      6.635     -0.608      0.548     -17.606       9.535\n",
      "3              6.5880     10.575      0.623      0.538     -15.041      28.217\n",
      "4             -2.7975      6.659     -0.420      0.678     -16.418      10.823\n",
      "==============================================================================\n",
      "Omnibus:                        7.425   Durbin-Watson:                   2.880\n",
      "Prob(Omnibus):                  0.024   Jarque-Bera (JB):                6.053\n",
      "Skew:                          -0.804   Prob(JB):                       0.0485\n",
      "Kurtosis:                       4.252   Cond. No.                         7.03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "******************************************************************************************************\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            DailyReturn   R-squared:                       0.029\n",
      "Model:                            OLS   Adj. R-squared:                 -0.100\n",
      "Method:                 Least Squares   F-statistic:                    0.2266\n",
      "Date:                Fri, 27 Apr 2018   Prob (F-statistic):              0.921\n",
      "Time:                        00:00:26   Log-Likelihood:                -124.56\n",
      "No. Observations:                  35   AIC:                             259.1\n",
      "Df Residuals:                      30   BIC:                             266.9\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.4494      1.733      1.413      0.168      -1.090       5.989\n",
      "0             -4.7582      7.364     -0.646      0.523     -19.797      10.281\n",
      "1             -3.7776      6.504     -0.581      0.566     -17.061       9.506\n",
      "2             -3.8088      6.147     -0.620      0.540     -16.363       8.746\n",
      "3              1.4861      7.892      0.188      0.852     -14.631      17.603\n",
      "==============================================================================\n",
      "Omnibus:                        6.482   Durbin-Watson:                   2.669\n",
      "Prob(Omnibus):                  0.039   Jarque-Bera (JB):                5.027\n",
      "Skew:                          -0.729   Prob(JB):                       0.0810\n",
      "Kurtosis:                       4.149   Cond. No.                         6.28\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "******************************************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1 day 5 clusters\n",
    "\n",
    "x_train = bitcoin_1day_5clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = bitcoin_1day_5clusters['DailyReturn']\n",
    "\n",
    "model_1_5 = sm.OLS(y_train.astype(float), x_train.astype(float))\n",
    "results_1_5 = model_1_5.fit()\n",
    "print(results_1_5.summary())\n",
    "print('\\n******************************************************************************************************\\n')\n",
    "\n",
    "#1 day 4 clusters\n",
    "\n",
    "x_train = bitcoin_1day_4clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = bitcoin_1day_4clusters['DailyReturn']\n",
    "\n",
    "model_1_4 = sm.OLS(y_train.astype(float), x_train.astype(float))\n",
    "results_1_4 = model_1_4.fit()\n",
    "print(results_1_4.summary())\n",
    "print('\\n******************************************************************************************************\\n')\n",
    "\n",
    "#3 day 5 clusters\n",
    "\n",
    "x_train = bitcoin_3day_5clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = bitcoin_3day_5clusters['DailyReturn']\n",
    "\n",
    "model_3_5 = sm.OLS(y_train.astype(float), x_train.astype(float))\n",
    "results_3_5 = model_3_5.fit()\n",
    "print(results_3_5.summary())\n",
    "print('\\n******************************************************************************************************\\n')\n",
    "pred = results_3_5.predict(x_train.astype(float))\n",
    "\n",
    "#3 day 4 clusters\n",
    "\n",
    "x_train = bitcoin_3day_4clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = bitcoin_3day_4clusters['DailyReturn']\n",
    "\n",
    "model_3_4 = sm.OLS(y_train.astype(float), x_train.astype(float))\n",
    "results_3_4 = model_3_4.fit()\n",
    "print(results_3_4.summary())\n",
    "print('\\n******************************************************************************************************\\n')\n",
    "\n",
    "bitcoin_1day_5clusters_binary = []\n",
    "\n",
    "for value in bitcoin_1day_5clusters['DailyReturn']:\n",
    "    if float(value) > 0:\n",
    "        value_binary = 1\n",
    "    else:\n",
    "        value_binary = 0\n",
    "        \n",
    "    bitcoin_1day_5clusters_binary.append(value_binary)\n",
    "    \n",
    "bitcoin_1day_4clusters_binary = []\n",
    "\n",
    "for value in bitcoin_1day_4clusters['DailyReturn']:\n",
    "    if float(value) > 0:\n",
    "        value_binary = 1\n",
    "    else:\n",
    "        value_binary = 0\n",
    "        \n",
    "    bitcoin_1day_4clusters_binary.append(value_binary)\n",
    "    \n",
    "bitcoin_3day_5clusters_binary = []\n",
    "\n",
    "for value in bitcoin_3day_5clusters['DailyReturn']:\n",
    "    if float(value) > 0:\n",
    "        value_binary = 1\n",
    "    else:\n",
    "        value_binary = 0\n",
    "        \n",
    "    bitcoin_3day_5clusters_binary.append(value_binary)\n",
    "    \n",
    "bitcoin_3day_4clusters_binary = []\n",
    "\n",
    "for value in bitcoin_3day_4clusters['DailyReturn']:\n",
    "    if float(value) > 0:\n",
    "        value_binary = 1\n",
    "    else:\n",
    "        value_binary = 0\n",
    "        \n",
    "    bitcoin_3day_4clusters_binary.append(value_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.21      0.59      0.31        17\n",
      "          1       0.88      0.58      0.70        90\n",
      "\n",
      "avg / total       0.77      0.58      0.64       107\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.31      0.58      0.41        26\n",
      "          1       0.81      0.59      0.69        81\n",
      "\n",
      "avg / total       0.69      0.59      0.62       107\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.08      1.00      0.14         1\n",
      "          1       1.00      0.65      0.79        34\n",
      "\n",
      "avg / total       0.97      0.66      0.77        35\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         0\n",
      "          1       1.00      0.63      0.77        35\n",
      "\n",
      "avg / total       1.00      0.63      0.77        35\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rushabh vakharia\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "LogReg = LogisticRegression()\n",
    "\n",
    "#1 day 5 clusters\n",
    "\n",
    "x_train = bitcoin_1day_5clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = bitcoin_1day_5clusters_binary\n",
    "\n",
    "model_1_5_log = LogReg.fit(x_train.astype(float),y_train)\n",
    "y_pred = model_1_5_log.predict(x_train.astype(float))\n",
    "print(classification_report(y_pred, y_train))\n",
    "\n",
    "#1 day 4 clusters\n",
    "\n",
    "x_train = bitcoin_1day_4clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = bitcoin_1day_4clusters_binary\n",
    "\n",
    "model_1_4_log = LogReg.fit(x_train.astype(float),y_train)\n",
    "y_pred = model_1_4_log.predict(x_train.astype(float))\n",
    "print(classification_report(y_pred, y_train))\n",
    "\n",
    "#3 day 5 clusters\n",
    "\n",
    "x_train = bitcoin_3day_5clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = bitcoin_3day_5clusters_binary\n",
    "\n",
    "model_3_5_log = LogReg.fit(x_train.astype(float),y_train)\n",
    "y_pred = model_3_5_log.predict(x_train.astype(float))\n",
    "print(classification_report(y_pred, y_train))\n",
    "\n",
    "#3 day 4 clusters\n",
    "\n",
    "x_train = bitcoin_3day_4clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = bitcoin_3day_4clusters_binary\n",
    "\n",
    "model_3_4_log = LogReg.fit(x_train.astype(float),y_train)\n",
    "y_pred = model_3_4_log.predict(x_train.astype(float))\n",
    "print(classification_report(y_pred, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputing the actual vs predicted returns in \"Actual_Predicted_1day_5clust_btc.csv\", \"Actual_Predicted_1day_4clust_btc.csv\", \"Actual_Predicted_3day_5clust_btc.csv\" and \"Actual_Predicted_3day_4clust_btc.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actual vs Predicted value comparison\n",
    "#1 day 5 clusters\n",
    "\n",
    "actual = bitcoin_1day_5clusters['DailyReturn'].tolist()\n",
    "predicted = pred.tolist()\n",
    "actual_predicted = list(zip(actual, predicted))\n",
    "\n",
    "actual_predicted = pd.DataFrame.from_records(actual_predicted, columns = [\"Actual Return\", \"Predicted Return\"])\n",
    "actual_predicted.to_csv(\"Actual_Predicted_1day_5clust_btc.csv\", encoding='utf-8')\n",
    "\n",
    "#1 day 4 clusters\n",
    "\n",
    "actual = bitcoin_1day_4clusters['DailyReturn'].tolist()\n",
    "predicted = pred.tolist()\n",
    "actual_predicted = list(zip(actual, predicted))\n",
    "\n",
    "actual_predicted = pd.DataFrame.from_records(actual_predicted, columns = [\"Actual Return\", \"Predicted Return\"])\n",
    "actual_predicted.to_csv(\"Actual_Predicted_1day_4clust_btc.csv\", encoding='utf-8')\n",
    "\n",
    "#3 day 5 clusters\n",
    "\n",
    "actual = bitcoin_3day_5clusters['DailyReturn'].tolist()\n",
    "predicted = pred.tolist()\n",
    "actual_predicted = list(zip(actual, predicted))\n",
    "\n",
    "actual_predicted = pd.DataFrame.from_records(actual_predicted, columns = [\"Actual Return\", \"Predicted Return\"])\n",
    "actual_predicted.to_csv(\"Actual_Predicted_3day_5clust_btc.csv\", encoding='utf-8')\n",
    "\n",
    "#3 day 4 clusters\n",
    "\n",
    "actual = bitcoin_3day_4clusters['DailyReturn'].tolist()\n",
    "predicted = pred.tolist()\n",
    "actual_predicted = list(zip(actual, predicted))\n",
    "\n",
    "actual_predicted = pd.DataFrame.from_records(actual_predicted, columns = [\"Actual Return\", \"Predicted Return\"])\n",
    "actual_predicted.to_csv(\"Actual_Predicted_3day_4clust_btc.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
