{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rushabh vakharia\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "c:\\users\\rushabh vakharia\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as expected_conditions\n",
    "import requests      \n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup  \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "import re, csv, string \n",
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import dateutil.parser\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defined functions to scrape Ethereum news from coindesk.com and to fetch headlines and date to be used later in sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate headlines out from 'all_data_raw' variable so that it can be written to separate columns in csv\n",
    "\n",
    "def get_headlines(data, index):\n",
    "    \n",
    "    headlines = []\n",
    "    \n",
    "    for row in data:\n",
    "        headlines.append(row[index])\n",
    "        \n",
    "    return headlines\n",
    "\n",
    "#Separate dates out from 'all_data' variable so that it can be written to separate columns in csv\n",
    "\n",
    "def get_date(data, index):\n",
    "    \n",
    "    date = []\n",
    "    \n",
    "    for row in data:\n",
    "        date.append(row[index])\n",
    "        \n",
    "    return date\n",
    "\n",
    "#Handle cleaning of one specific date format\n",
    "\n",
    "def tokenize_date(t):\n",
    "    \n",
    "    pattern=r'[a-zA-Z{3}]+[.\\s]+[\\d{1,2}\\,\\s]+[\\d{4}]+'\n",
    "    \n",
    "    date_pattern = nltk.regexp_tokenize(t, pattern)\n",
    "    \n",
    "    return date_pattern\n",
    "\n",
    "#Get Ethereum news from coindesk.com\n",
    "\n",
    "def get_ethereumNews():\n",
    "    \n",
    "    new=[]\n",
    "    url = \"https://www.coindesk.com/category/technology-news/ethereum-technology-news/\"\n",
    "    executable_path = 'C:/Users/Rushabh Vakharia/geckodriver-v0.19.1-win64/geckodriver'\n",
    "    service_log_path = 'C:/Users/Rushabh Vakharia/geckodriver-v0.19.1-win64/driver'\n",
    "    \n",
    "    driver = webdriver.Firefox(executable_path=executable_path)\n",
    "\n",
    "    driver.get(url)\n",
    "    html = driver.page_source.encode('utf-8')\n",
    "    more_path='div#byscripts_ajax_posts_loader_trigger'\n",
    "    longItemStr=[]\n",
    "    \n",
    "    for i in range(50):\n",
    "        \n",
    "        WebDriverWait(driver,10).until(expected_conditions.visibility_of_element_located((By.CSS_SELECTOR, more_path)))\n",
    "        driver.execute_script(\"arguments[0].click();\", WebDriverWait(driver,10).until(expected_conditions.visibility_of_element_located((By.CSS_SELECTOR, more_path))))\n",
    "    \n",
    "        headline=driver.find_elements_by_class_name(\"fade\")  \n",
    "        dates=driver.find_elements_by_css_selector(\"time\")\n",
    "        print(\"Getting page number \"+str(i))\n",
    "\n",
    "        for head in headline:\n",
    "            if head.text!='' and head.text not in longItemStr:  \n",
    "                longItemStr.append(head.text)\n",
    "                    \n",
    "        for date in dates:\n",
    "            #if date not in new:\n",
    "            date=str(dateutil.parser.parse(date.text).date())\n",
    "            date=datetime.strptime(date,\"%Y-%m-%d\").strftime(\"%m/%d/%Y\")\n",
    "            dt_obj = datetime.strptime(date,'%m/%d/%Y')\n",
    "            date= datetime.strftime(dt_obj,'%b %d, %Y')\n",
    "            #dt_obj = datetime.strptime(date,'%m/%d/%Y')\n",
    "            new.append(date)\n",
    "                 \n",
    "    raw_data = zip(longItemStr, new)\n",
    "    driver.quit()\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling the functions defined above to scrape the data and then store it in the list \"all_data_raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping ethereum news from coindesk.com\n",
      "*******************************************\n",
      "\n",
      "Getting page number 0\n",
      "Getting page number 1\n",
      "Getting page number 2\n",
      "Getting page number 3\n",
      "Getting page number 4\n",
      "Getting page number 5\n",
      "Getting page number 6\n",
      "Getting page number 7\n",
      "Getting page number 8\n",
      "Getting page number 9\n",
      "Getting page number 10\n",
      "Getting page number 11\n",
      "Getting page number 12\n",
      "Getting page number 13\n",
      "Getting page number 14\n",
      "Getting page number 15\n",
      "Getting page number 16\n",
      "Getting page number 17\n",
      "Getting page number 18\n",
      "Getting page number 19\n",
      "Getting page number 20\n",
      "Getting page number 21\n",
      "Getting page number 22\n",
      "Getting page number 23\n",
      "Getting page number 24\n",
      "Getting page number 25\n",
      "Getting page number 26\n",
      "Getting page number 27\n",
      "Getting page number 28\n",
      "Getting page number 29\n",
      "Getting page number 30\n",
      "Getting page number 31\n",
      "Getting page number 32\n",
      "Getting page number 33\n",
      "Getting page number 34\n",
      "Getting page number 35\n",
      "Getting page number 36\n",
      "Getting page number 37\n",
      "Getting page number 38\n",
      "Getting page number 39\n",
      "Getting page number 40\n",
      "Getting page number 41\n",
      "Getting page number 42\n",
      "Getting page number 43\n",
      "Getting page number 44\n",
      "Getting page number 45\n",
      "Getting page number 46\n",
      "Getting page number 47\n",
      "Getting page number 48\n",
      "Getting page number 49\n",
      "\n",
      "Done scraping all data and stored in list 'all_data_raw'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"Scraping ethereum news from coindesk.com\")\n",
    "    print(\"*******************************************\\n\")\n",
    "    ethereum_raw = get_ethereumNews()\n",
    "    \n",
    "    all_data_raw = []\n",
    "    all_data_raw.extend(list(ethereum_raw))\n",
    "    \n",
    "    print(\"\\nDone scraping all data and stored in list 'all_data_raw'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying the headlines as positive, negative and neutral and printing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral headlines:  235\n",
      "Somewhat negative headlines:  46\n",
      "Very negative headlines:  7\n",
      "Somewhat positive headlines:  71\n",
      "Very positive headlines:  10\n",
      "Total number of headlines:  369\n"
     ]
    }
   ],
   "source": [
    "#Filter out headlines without these keywords\n",
    "\n",
    "keywords = ['ethereum', 'cryptocurrency','cryptocurrencies', 'crypto', 'blockchain', 'blockchains']\n",
    "\n",
    "#Get headlines and dates as individual lists\n",
    "\n",
    "headlines = get_headlines(all_data_raw,0)\n",
    "dates = get_date(all_data_raw,1)\n",
    "sid = SentimentIntensityAnalyzer()       \n",
    "compound = []\n",
    "\n",
    "for head in headlines:\n",
    "    head_lower = head.lower()\n",
    "    ss = sid.polarity_scores(head_lower)\n",
    "    compound.append(ss['compound'])\n",
    "    \n",
    "#Counting the number of headlines in each sentiment class\n",
    "\n",
    "neutral = []\n",
    "somewhat_negative = []\n",
    "somewhat_positive = []\n",
    "very_negative = []\n",
    "very_positive = []\n",
    "\n",
    "for index, score in enumerate(compound):\n",
    "    \n",
    "    if score > -0.20 and score < 0.20:\n",
    "        neutral.append(score)\n",
    "        \n",
    "    elif score > -0.60 and score < -0.20:\n",
    "        somewhat_negative.append(score)\n",
    "        \n",
    "    elif score > 0.20 and score < 0.60:\n",
    "        somewhat_positive.append(score)\n",
    "        \n",
    "    elif score <= -0.60:\n",
    "        very_negative.append(score)\n",
    "        \n",
    "    else:\n",
    "        very_positive.append(score)\n",
    "\n",
    "print('Neutral headlines: ', len(neutral))\n",
    "print('Somewhat negative headlines: ', len(somewhat_negative))\n",
    "print(\"Very negative headlines: \", len(very_negative))\n",
    "print('Somewhat positive headlines: ', len(somewhat_positive))\n",
    "print(\"Very positive headlines: \", len(very_positive))\n",
    "print('Total number of headlines: ', len(compound))\n",
    "\n",
    "data_with_sentiment = list(zip(dates, headlines, compound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing, lemmatizing and removing frequent keywords to improve clustering performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize and lemmatize headlines\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    \n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    \n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    \n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    \n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def lemmatize(document):\n",
    "    \n",
    "    pattern=r'[a-zA-Z]+[a-zA-Z\\-]+[a-zA-Z]'      \n",
    "    tokens=nltk.regexp_tokenize(document, pattern)\n",
    "    tagged_tokens=nltk.pos_tag(tokens)\n",
    "    wordnet_lemmatizer=WordNetLemmatizer()\n",
    "    \n",
    "    le_words=[wordnet_lemmatizer.lemmatize(word, get_wordnet_pos(tag)) \\\n",
    "              for (word, tag) in tagged_tokens \\\n",
    "              if word not in stop_words and \\\n",
    "              word not in string.punctuation]\n",
    "    \n",
    "    le_words = list(set(le_words))\n",
    "    return le_words\n",
    "\n",
    "lem_headlines = []\n",
    "\n",
    "for headline in headlines:\n",
    "    \n",
    "    lem_headline = lemmatize(headline)\n",
    "    lem_headlines.append(lem_headline)\n",
    "\n",
    "lem_data_with_sentiment = list(zip(dates, headlines, lem_headlines, compound))\n",
    "\n",
    "#Function to remove frequent keywords from lemmatized headline tokens to improve clustering performance\n",
    "\n",
    "def remove_keywords(data):\n",
    "    for row in data:\n",
    "        lem_list = row[2]\n",
    "        for word in lem_list:\n",
    "            if word in keywords:\n",
    "                lem_list.remove(word)\n",
    "        \n",
    "    return data\n",
    "\n",
    "#Preparing data for LDA clustering\n",
    "\n",
    "data_for_clustering = remove_keywords(lem_data_with_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LDA clustering model for 5 clusters and storing the data in \"Cleaned_data_final_5clusters_eth.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.086*\"Ethereum\" + 0.039*\"Blockchain\" + 0.015*\"Fork\" + 0.014*\"New\" + 0.009*\"Developers\"'), (1, '0.038*\"Ethereum\" + 0.030*\"Ether\" + 0.015*\"Blockchain\" + 0.011*\"Parity\" + 0.011*\"Bitcoin\"'), (2, '0.051*\"Ethereum\" + 0.013*\"The\" + 0.009*\"Cryptocurrency\" + 0.009*\"Now\" + 0.008*\"Blockchain\"'), (3, '0.051*\"Ethereum\" + 0.019*\"New\" + 0.016*\"Blockchain\" + 0.009*\"Fork\" + 0.008*\"The\"'), (4, '0.061*\"Ethereum\" + 0.025*\"The\" + 0.020*\"Vitalik\" + 0.019*\"DAO\" + 0.015*\"Buterin\"')]\n",
      "\n",
      "All data cleaned, clustered and written to csv file 'Cleaned_data_final_5clusters_eth.csv'\n"
     ]
    }
   ],
   "source": [
    "#Building LDA Model for 5 clusters\n",
    "\n",
    "headline = get_headlines(data_for_clustering,2)\n",
    "dictionary = corpora.Dictionary(headline)\n",
    "corpus = [dictionary.doc2bow(row) for row in headline]\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word = dictionary, passes=20)\n",
    "print(ldamodel.print_topics(num_topics=5, num_words=5))\n",
    "\n",
    "#Classifying clusters and mapping them to headlines\n",
    "\n",
    "cluster = ldamodel[corpus]\n",
    "clusters = []\n",
    "list_ = []\n",
    "\n",
    "for t in cluster:\n",
    "    list_ = (list(zip(*t))[1])\n",
    "    clusters.append(list_.index(max(list_)))\n",
    "\n",
    "date, headline, tokenized_headline, sentiment = zip(*data_for_clustering)\n",
    "clean_data_5clusters = list(zip(date, headline, tokenized_headline, sentiment, clusters))\n",
    "\n",
    "#Displaying the headlines within each cluster\n",
    "\n",
    "filter(lambda x: x[1] == 0, clean_data_5clusters)\n",
    "filter(lambda x: x[1] == 1, clean_data_5clusters)\n",
    "filter(lambda x: x[1] == 2, clean_data_5clusters)\n",
    "filter(lambda x: x[1] == 3, clean_data_5clusters)\n",
    "filter(lambda x: x[1] == 4, clean_data_5clusters)\n",
    "\n",
    "#Writing cleaned data to csv for 5 clusters\n",
    "\n",
    "data_frame_5clusters = pd.DataFrame.from_records(clean_data_5clusters, columns = [\"Date\", \"Headline\", \"Tokenized Headline\", \"Sentiment\", \"Cluster\"])\n",
    "data_frame_5clusters.to_csv(\"Cleaned_data_final_5clusters_eth.csv\", encoding='utf-8')\n",
    "print(\"\\nAll data cleaned, clustered and written to csv file 'Cleaned_data_final_5clusters_eth.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LDA clustering model for 4 clusters and storing the data in \"Cleaned_data_final_4clusters_eth.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.055*\"Ethereum\" + 0.021*\"Blockchain\" + 0.016*\"Ether\" + 0.011*\"New\" + 0.011*\"The\"'), (1, '0.034*\"Ethereum\" + 0.017*\"Blockchain\" + 0.013*\"Over\" + 0.012*\"Million\" + 0.010*\"DAO\"'), (2, '0.074*\"Ethereum\" + 0.026*\"Blockchain\" + 0.012*\"DAO\" + 0.012*\"The\" + 0.011*\"Fork\"'), (3, '0.070*\"Ethereum\" + 0.013*\"Blockchain\" + 0.013*\"Launch\" + 0.010*\"New\" + 0.009*\"Are\"')]\n",
      "\n",
      "All data cleaned, clustered and written to csv file 'Cleaned_data_final_4clusters_eth.csv'\n"
     ]
    }
   ],
   "source": [
    "#Building LDA Model for 4 clusters\n",
    "\n",
    "headline = get_headlines(data_for_clustering,2)\n",
    "dictionary = corpora.Dictionary(headline)\n",
    "corpus = [dictionary.doc2bow(row) for row in headline]\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=4, id2word = dictionary, passes=20)\n",
    "print(ldamodel.print_topics(num_topics=4, num_words=5))\n",
    "\n",
    "#Classifying clusters and mapping them to headlines\n",
    "\n",
    "cluster = ldamodel[corpus]\n",
    "clusters = []\n",
    "list_ = []\n",
    "\n",
    "for t in cluster:\n",
    "    list_ = (list(zip(*t))[1])\n",
    "    clusters.append(list_.index(max(list_)))\n",
    "\n",
    "date, headline, tokenized_headline, sentiment = zip(*data_for_clustering)\n",
    "clean_data_5clusters = list(zip(date, headline, tokenized_headline, sentiment, clusters))\n",
    "\n",
    "#Displaying the headlines within each cluster\n",
    "\n",
    "filter(lambda x: x[1] == 0, clean_data_5clusters)\n",
    "filter(lambda x: x[1] == 1, clean_data_5clusters)\n",
    "filter(lambda x: x[1] == 2, clean_data_5clusters)\n",
    "filter(lambda x: x[1] == 3, clean_data_5clusters)\n",
    "\n",
    "#Writing cleaned data to csv for 4 clusters\n",
    "\n",
    "data_frame_4clusters = pd.DataFrame.from_records(clean_data_5clusters, columns = [\"Date\", \"Headline\", \"Tokenized Headline\", \"Sentiment\", \"Cluster\"])\n",
    "data_frame_4clusters.to_csv(\"Cleaned_data_final_4clusters_eth.csv\", encoding='utf-8')\n",
    "print(\"\\nAll data cleaned, clustered and written to csv file 'Cleaned_data_final_4clusters_eth.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating pivot tables based on dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating pivot tables by date\n",
    "\n",
    "pivot_table_5 = pd.pivot_table(data_frame_5clusters, values = 'Sentiment', index = 'Date', columns = 'Cluster', aggfunc = np.sum, fill_value = 0)\n",
    "pivot_table_4 = pd.pivot_table(data_frame_4clusters, values = 'Sentiment', index = 'Date', columns = 'Cluster', aggfunc = np.sum, fill_value = 0)\n",
    "\n",
    "pivot_table_5 = pivot_table_5.rename_axis(None, axis =1).reset_index()\n",
    "pivot_table_4 = pivot_table_4.rename_axis(None, axis =1).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging pivot tables with ethereum pricing data stored in files \"Ethereum1Day.csv\" and \"Ethereum3Day.csv\". These files have data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge pivot tables with bitcoin pricing data\n",
    "\n",
    "with open(\"Ethereum1Day.csv\", \"r\") as f:\n",
    "    \n",
    "    reader=csv.reader(f, delimiter=',') \n",
    "    ethereum_1day = [row for row in reader]\n",
    "    del ethereum_1day[0]\n",
    "\n",
    "date_list = []\n",
    "\n",
    "for date in get_date(ethereum_1day,0):\n",
    "    date = datetime.strptime(date, '%m/%d/%Y')\n",
    "    date = datetime.strftime(date,'%b %d, %Y')\n",
    "    date_list.append(date)\n",
    "    \n",
    "date, dailyreturn = zip(*ethereum_1day)\n",
    "ethereum_1day = zip(date_list,dailyreturn)\n",
    "    \n",
    "ethereum_1day = pd.DataFrame.from_records(ethereum_1day, columns = [\"Date\",\"DailyReturn\"])\n",
    "ethereum_1day\n",
    "\n",
    "ethereum_1day_5clusters = ethereum_1day.merge(pivot_table_5, on = 'Date', how = 'left')\n",
    "ethereum_1day_5clusters = ethereum_1day_5clusters.dropna()\n",
    "\n",
    "ethereum_1day_4clusters = ethereum_1day.merge(pivot_table_4, on = 'Date', how = 'left')\n",
    "ethereum_1day_4clusters = ethereum_1day_4clusters.dropna()\n",
    "\n",
    "with open(\"Ethereum3Day.csv\", \"r\") as f:\n",
    "    \n",
    "    reader=csv.reader(f, delimiter=',') \n",
    "    ethereum_3day = [row for row in reader]\n",
    "    del ethereum_3day[0]\n",
    "\n",
    "date_list = []\n",
    "\n",
    "for date in get_date(ethereum_3day,0):\n",
    "    date = datetime.strptime(date, '%m/%d/%Y')\n",
    "    date = datetime.strftime(date,'%b %d, %Y')\n",
    "    date_list.append(date)\n",
    "    \n",
    "date, dailyreturn = zip(*ethereum_3day)\n",
    "ethereum_3day = zip(date_list,dailyreturn)\n",
    "    \n",
    "ethereum_3day = pd.DataFrame.from_records(ethereum_3day, columns = [\"Date\",\"DailyReturn\"])\n",
    "\n",
    "ethereum_3day_5clusters = ethereum_3day.merge(pivot_table_5, on = 'Date', how = 'left')\n",
    "ethereum_3day_5clusters = ethereum_3day_5clusters.dropna()\n",
    "\n",
    "ethereum_3day_4clusters = ethereum_3day.merge(pivot_table_4, on = 'Date', how = 'left')\n",
    "ethereum_3day_4clusters = ethereum_3day_4clusters.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            DailyReturn   R-squared:                       0.155\n",
      "Model:                            OLS   Adj. R-squared:                 -0.043\n",
      "Method:                 Least Squares   F-statistic:                    0.7825\n",
      "Date:                Fri, 27 Apr 2018   Prob (F-statistic):              0.552\n",
      "Time:                        00:09:56   Log-Likelihood:                -70.319\n",
      "No. Observations:                  22   AIC:                             150.6\n",
      "Df Residuals:                      17   BIC:                             156.1\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          3.3506      1.784      1.878      0.078      -0.414       7.115\n",
      "0              2.1143      6.524      0.324      0.750     -11.649      15.878\n",
      "1             32.6921     22.163      1.475      0.158     -14.067      79.452\n",
      "2             15.3751     16.168      0.951      0.355     -18.736      49.486\n",
      "3                   0          0        nan        nan           0           0\n",
      "4             -4.8008      7.289     -0.659      0.519     -20.179      10.577\n",
      "==============================================================================\n",
      "Omnibus:                        0.428   Durbin-Watson:                   2.184\n",
      "Prob(Omnibus):                  0.807   Jarque-Bera (JB):                0.540\n",
      "Skew:                          -0.051   Prob(JB):                        0.763\n",
      "Kurtosis:                       2.239   Cond. No.                          inf\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is      0. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "\n",
      "******************************************************************************************************\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            DailyReturn   R-squared:                       0.021\n",
      "Model:                            OLS   Adj. R-squared:                 -0.210\n",
      "Method:                 Least Squares   F-statistic:                   0.09022\n",
      "Date:                Fri, 27 Apr 2018   Prob (F-statistic):              0.984\n",
      "Time:                        00:09:56   Log-Likelihood:                -71.947\n",
      "No. Observations:                  22   AIC:                             153.9\n",
      "Df Residuals:                      17   BIC:                             159.3\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.3843      1.792      1.330      0.201      -1.397       6.166\n",
      "0             -8.5298     21.477     -0.397      0.696     -53.842      36.782\n",
      "1             -0.6318     11.897     -0.053      0.958     -25.731      24.468\n",
      "2             -1.5582      6.801     -0.229      0.822     -15.908      12.791\n",
      "3             -3.4099      7.929     -0.430      0.673     -20.138      13.318\n",
      "==============================================================================\n",
      "Omnibus:                        1.992   Durbin-Watson:                   1.797\n",
      "Prob(Omnibus):                  0.369   Jarque-Bera (JB):                1.273\n",
      "Skew:                           0.312   Prob(JB):                        0.529\n",
      "Kurtosis:                       2.001   Cond. No.                         14.8\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "******************************************************************************************************\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            DailyReturn   R-squared:                       0.413\n",
      "Model:                            OLS   Adj. R-squared:                  0.275\n",
      "Method:                 Least Squares   F-statistic:                     2.987\n",
      "Date:                Fri, 27 Apr 2018   Prob (F-statistic):             0.0489\n",
      "Time:                        00:09:56   Log-Likelihood:                -82.137\n",
      "No. Observations:                  22   AIC:                             174.3\n",
      "Df Residuals:                      17   BIC:                             179.7\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          8.0745      3.053      2.645      0.017       1.633      14.516\n",
      "0             27.8749     11.163      2.497      0.023       4.323      51.426\n",
      "1             61.5576     37.924      1.623      0.123     -18.455     141.570\n",
      "2             36.1265     27.666      1.306      0.209     -22.243      94.496\n",
      "3                   0          0        nan        nan           0           0\n",
      "4            -17.5973     12.472     -1.411      0.176     -43.911       8.717\n",
      "==============================================================================\n",
      "Omnibus:                        2.876   Durbin-Watson:                   1.500\n",
      "Prob(Omnibus):                  0.237   Jarque-Bera (JB):                1.338\n",
      "Skew:                          -0.527   Prob(JB):                        0.512\n",
      "Kurtosis:                       3.591   Cond. No.                          inf\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is      0. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "\n",
      "******************************************************************************************************\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            DailyReturn   R-squared:                       0.115\n",
      "Model:                            OLS   Adj. R-squared:                 -0.093\n",
      "Method:                 Least Squares   F-statistic:                    0.5539\n",
      "Date:                Fri, 27 Apr 2018   Prob (F-statistic):              0.699\n",
      "Time:                        00:09:56   Log-Likelihood:                -86.645\n",
      "No. Observations:                  22   AIC:                             183.3\n",
      "Df Residuals:                      17   BIC:                             188.7\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          5.0142      3.496      1.434      0.170      -2.362      12.390\n",
      "0             43.5167     41.889      1.039      0.313     -44.862     131.896\n",
      "1              5.5296     23.204      0.238      0.814     -43.426      54.485\n",
      "2              1.2926     13.266      0.097      0.924     -26.696      29.281\n",
      "3             17.7417     15.465      1.147      0.267     -14.886      50.369\n",
      "==============================================================================\n",
      "Omnibus:                        1.157   Durbin-Watson:                   1.360\n",
      "Prob(Omnibus):                  0.561   Jarque-Bera (JB):                0.369\n",
      "Skew:                          -0.300   Prob(JB):                        0.832\n",
      "Kurtosis:                       3.208   Cond. No.                         14.8\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "******************************************************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rushabh vakharia\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1471: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  return np.sqrt(eigvals[0]/eigvals[-1])\n",
      "c:\\users\\rushabh vakharia\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\statsmodels\\base\\model.py:1036: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return self.params / self.bse\n",
      "c:\\users\\rushabh vakharia\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "c:\\users\\rushabh vakharia\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "c:\\users\\rushabh vakharia\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1821: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    }
   ],
   "source": [
    "#1 day 5 clusters\n",
    "\n",
    "x_train = ethereum_1day_5clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = ethereum_1day_5clusters['DailyReturn']\n",
    "\n",
    "model_1_5 = sm.OLS(y_train.astype(float), x_train.astype(float))\n",
    "results_1_5 = model_1_5.fit()\n",
    "print(results_1_5.summary())\n",
    "print('\\n******************************************************************************************************\\n')\n",
    "\n",
    "#1 day 4 clusters\n",
    "\n",
    "x_train = ethereum_1day_4clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = ethereum_1day_4clusters['DailyReturn']\n",
    "\n",
    "model_1_4 = sm.OLS(y_train.astype(float), x_train.astype(float))\n",
    "results_1_4 = model_1_4.fit()\n",
    "print(results_1_4.summary())\n",
    "print('\\n******************************************************************************************************\\n')\n",
    "\n",
    "#3 day 5 clusters\n",
    "\n",
    "x_train = ethereum_3day_5clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = ethereum_3day_5clusters['DailyReturn']\n",
    "\n",
    "model_3_5 = sm.OLS(y_train.astype(float), x_train.astype(float))\n",
    "results_3_5 = model_3_5.fit()\n",
    "print(results_3_5.summary())\n",
    "print('\\n******************************************************************************************************\\n')\n",
    "pred = results_3_5.predict(x_train.astype(float))\n",
    "\n",
    "#3 day 4 clusters\n",
    "\n",
    "x_train = ethereum_3day_4clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = ethereum_3day_5clusters['DailyReturn']\n",
    "\n",
    "model_3_4 = sm.OLS(y_train.astype(float), x_train.astype(float))\n",
    "results_3_4 = model_3_4.fit()\n",
    "print(results_3_4.summary())\n",
    "print('\\n******************************************************************************************************\\n')\n",
    "\n",
    "ethereum_1day_5clusters_binary = []\n",
    "\n",
    "for value in ethereum_1day_5clusters['DailyReturn']:\n",
    "    if float(value) > 0:\n",
    "        value_binary = 1\n",
    "    else:\n",
    "        value_binary = 0\n",
    "        \n",
    "    ethereum_1day_5clusters_binary.append(value_binary)\n",
    "    \n",
    "ethereum_1day_4clusters_binary = []\n",
    "\n",
    "for value in ethereum_1day_4clusters['DailyReturn']:\n",
    "    if float(value) > 0:\n",
    "        value_binary = 1\n",
    "    else:\n",
    "        value_binary = 0\n",
    "        \n",
    "    ethereum_1day_4clusters_binary.append(value_binary)\n",
    "    \n",
    "ethereum_3day_5clusters_binary = []\n",
    "\n",
    "for value in ethereum_3day_5clusters['DailyReturn']:\n",
    "    if float(value) > 0:\n",
    "        value_binary = 1\n",
    "    else:\n",
    "        value_binary = 0\n",
    "        \n",
    "    ethereum_3day_5clusters_binary.append(value_binary)\n",
    "    \n",
    "ethereum_3day_4clusters_binary = []\n",
    "\n",
    "for value in ethereum_3day_4clusters['DailyReturn']:\n",
    "    if float(value) > 0:\n",
    "        value_binary = 1\n",
    "    else:\n",
    "        value_binary = 0\n",
    "        \n",
    "    ethereum_3day_4clusters_binary.append(value_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         0\n",
      "          1       1.00      0.59      0.74        22\n",
      "\n",
      "avg / total       1.00      0.59      0.74        22\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.11      1.00      0.20         1\n",
      "          1       1.00      0.62      0.76        21\n",
      "\n",
      "avg / total       0.96      0.64      0.74        22\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         0\n",
      "          1       1.00      0.73      0.84        22\n",
      "\n",
      "avg / total       1.00      0.73      0.84        22\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         0\n",
      "          1       1.00      0.73      0.84        22\n",
      "\n",
      "avg / total       1.00      0.73      0.84        22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rushabh vakharia\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "LogReg = LogisticRegression()\n",
    "\n",
    "#1 day 5 clusters\n",
    "\n",
    "x_train = ethereum_1day_5clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = ethereum_1day_5clusters_binary\n",
    "\n",
    "model_1_5_log = LogReg.fit(x_train.astype(float),y_train)\n",
    "y_pred = model_1_5_log.predict(x_train.astype(float))\n",
    "print(classification_report(y_pred, y_train))\n",
    "\n",
    "#1 day 4 clusters\n",
    "\n",
    "x_train = ethereum_1day_4clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = ethereum_1day_4clusters_binary\n",
    "\n",
    "model_1_4_log = LogReg.fit(x_train.astype(float),y_train)\n",
    "y_pred = model_1_4_log.predict(x_train.astype(float))\n",
    "print(classification_report(y_pred, y_train))\n",
    "\n",
    "#3 day 5 clusters\n",
    "\n",
    "x_train = ethereum_3day_5clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = ethereum_3day_5clusters_binary\n",
    "\n",
    "model_3_5_log = LogReg.fit(x_train.astype(float),y_train)\n",
    "y_pred = model_3_5_log.predict(x_train.astype(float))\n",
    "print(classification_report(y_pred, y_train))\n",
    "\n",
    "#3 day 4 clusters\n",
    "\n",
    "x_train = ethereum_3day_4clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = ethereum_3day_4clusters_binary\n",
    "\n",
    "model_3_4_log = LogReg.fit(x_train.astype(float),y_train)\n",
    "y_pred = model_3_4_log.predict(x_train.astype(float))\n",
    "print(classification_report(y_pred, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputing the actual vs predicted returns in \"Actual_Predicted_1day_5clust_eth.csv\", \"Actual_Predicted_1day_4clust_eth.csv\", \"Actual_Predicted_3day_5clust_eth.csv\" and \"Actual_Predicted_3day_4clust_eth.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actual vs Predicted value comparison\n",
    "#1 day 5 clusters\n",
    "\n",
    "actual = ethereum_1day_5clusters['DailyReturn'].tolist()\n",
    "predicted = pred.tolist()\n",
    "actual_predicted = list(zip(actual, predicted))\n",
    "\n",
    "actual_predicted = pd.DataFrame.from_records(actual_predicted, columns = [\"Actual Return\", \"Predicted Return\"])\n",
    "actual_predicted.to_csv(\"Actual_Predicted_1day_5clust_eth.csv\", encoding='utf-8')\n",
    "\n",
    "#1 day 4 clusters\n",
    "\n",
    "actual = ethereum_1day_4clusters['DailyReturn'].tolist()\n",
    "predicted = pred.tolist()\n",
    "actual_predicted = list(zip(actual, predicted))\n",
    "\n",
    "actual_predicted = pd.DataFrame.from_records(actual_predicted, columns = [\"Actual Return\", \"Predicted Return\"])\n",
    "actual_predicted.to_csv(\"Actual_Predicted_1day_4clust_eth.csv\", encoding='utf-8')\n",
    "\n",
    "#3 day 5 clusters\n",
    "\n",
    "actual = ethereum_3day_5clusters['DailyReturn'].tolist()\n",
    "predicted = pred.tolist()\n",
    "actual_predicted = list(zip(actual, predicted))\n",
    "\n",
    "actual_predicted = pd.DataFrame.from_records(actual_predicted, columns = [\"Actual Return\", \"Predicted Return\"])\n",
    "actual_predicted.to_csv(\"Actual_Predicted_3day_5clust_eth.csv\", encoding='utf-8')\n",
    "\n",
    "#3 day 4 clusters\n",
    "\n",
    "actual = ethereum_3day_4clusters['DailyReturn'].tolist()\n",
    "predicted = pred.tolist()\n",
    "actual_predicted = list(zip(actual, predicted))\n",
    "\n",
    "actual_predicted = pd.DataFrame.from_records(actual_predicted, columns = [\"Actual Return\", \"Predicted Return\"])\n",
    "actual_predicted.to_csv(\"Actual_Predicted_3day_4clust_eth.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
