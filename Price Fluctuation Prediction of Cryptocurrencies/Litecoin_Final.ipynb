{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rushabh vakharia\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "c:\\users\\rushabh vakharia\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import requests      \n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup  \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "import re, csv, string \n",
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import dateutil.parser\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defined functions to scrape Litecoin news from litecoinnews.io and to fetch headlines and date to be used later in sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate headlines out from 'all_data_raw' variable so that it can be written to separate columns in csv\n",
    "\n",
    "def get_headlines(data, index):\n",
    "    \n",
    "    headlines = []\n",
    "    \n",
    "    for row in data:\n",
    "        headlines.append(row[index])\n",
    "        \n",
    "    return headlines\n",
    "\n",
    "#Separate dates out from 'all_data' variable so that it can be written to separate columns in csv\n",
    "\n",
    "def get_date(data, index):\n",
    "    \n",
    "    date = []\n",
    "    \n",
    "    for row in data:\n",
    "        date.append(row[index])\n",
    "        \n",
    "    return date\n",
    "\n",
    "#Handle cleaning of one specific date format\n",
    "\n",
    "def tokenize_date(t):\n",
    "    \n",
    "    pattern=r'[a-zA-Z{3}]+[.\\s]+[\\d{1,2}\\,\\s]+[\\d{4}]+' \n",
    "    \n",
    "    date_pattern = nltk.regexp_tokenize(t, pattern)\n",
    "    \n",
    "    return date_pattern\n",
    "\n",
    "#Get Litecoin news from litecoinnews.io\n",
    "\n",
    "def get_litecoinNews():\n",
    "    \n",
    "    headlines=[]  #List to store headlines\n",
    "    dates = []    #List to store date\n",
    "    raw_headlines = []\n",
    "    page_number = 1\n",
    "    page_url=\"https://litecoinnews.io/\"\n",
    "    raw_data=[]\n",
    "    final=[]\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'}\n",
    "    \n",
    "    while page_url!=None:\n",
    "        \n",
    "        page = requests.get(page_url,headers=headers) \n",
    "        \n",
    "        if page.status_code!=200:    \n",
    "            page_url=None\n",
    "        else:                       \n",
    "            page = requests.get(page_url,headers=headers) \n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            divs = soup.findAll(\"div\", {\"class\": \"main-post\"})\n",
    "\n",
    "            for idx, div in enumerate(divs):\n",
    "                \n",
    "                headline=None  \n",
    "                date=None\n",
    "                \n",
    "                headline=div.select(\"h5.post-title\")[0].get_text()  \n",
    "                \n",
    "                date=div.select(\"time\")[0].get_text()\n",
    "                date=str(dateutil.parser.parse(date).date())\n",
    "                date=datetime.strptime(date,\"%Y-%m-%d\").strftime(\"%m/%d/%Y\")\n",
    "                dt_obj = datetime.strptime(date,'%m/%d/%Y')\n",
    "                date=date=datetime.strftime(dt_obj,'%b %d, %Y')\n",
    "                \n",
    "                raw_data.append((headline.replace('\\n',\"\"),date))\n",
    "                \n",
    "        tag=soup.find(\"ul\",class_=\"pagination\")\n",
    "        new=tag.find(\"li\", {\"class\": \"active\"})\n",
    "        divTag=((new).find_next_sibling(\"li\"))\n",
    "        print(page_url)\n",
    "        \n",
    "        if(divTag!=None):\n",
    "            page_url=divTag.select(\"a\")[0].get('href')\n",
    "        else:\n",
    "            page_url=None            \n",
    "\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling the functions defined above to scrape the data and then store it in the list \"all_data_raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping litecoin news from litecoinnews.io\n",
      "*******************************************\n",
      "\n",
      "https://litecoinnews.io/\n",
      "https://litecoinnews.io/?page=2\n",
      "https://litecoinnews.io/?page=3\n",
      "https://litecoinnews.io/?page=4\n",
      "https://litecoinnews.io/?page=5\n",
      "https://litecoinnews.io/?page=6\n",
      "https://litecoinnews.io/?page=7\n",
      "https://litecoinnews.io/?page=8\n",
      "https://litecoinnews.io/?page=9\n",
      "https://litecoinnews.io/?page=10\n",
      "https://litecoinnews.io/?page=11\n",
      "https://litecoinnews.io/?page=12\n",
      "https://litecoinnews.io/?page=13\n",
      "https://litecoinnews.io/?page=14\n",
      "https://litecoinnews.io/?page=15\n",
      "https://litecoinnews.io/?page=16\n",
      "https://litecoinnews.io/?page=17\n",
      "https://litecoinnews.io/?page=18\n",
      "https://litecoinnews.io/?page=19\n",
      "https://litecoinnews.io/?page=20\n",
      "https://litecoinnews.io/?page=21\n",
      "https://litecoinnews.io/?page=22\n",
      "https://litecoinnews.io/?page=23\n",
      "https://litecoinnews.io/?page=24\n",
      "https://litecoinnews.io/?page=25\n",
      "https://litecoinnews.io/?page=26\n",
      "https://litecoinnews.io/?page=27\n",
      "https://litecoinnews.io/?page=28\n",
      "https://litecoinnews.io/?page=29\n",
      "https://litecoinnews.io/?page=30\n",
      "https://litecoinnews.io/?page=31\n",
      "https://litecoinnews.io/?page=32\n",
      "https://litecoinnews.io/?page=33\n",
      "https://litecoinnews.io/?page=34\n",
      "https://litecoinnews.io/?page=35\n",
      "https://litecoinnews.io/?page=36\n",
      "https://litecoinnews.io/?page=37\n",
      "https://litecoinnews.io/?page=38\n",
      "https://litecoinnews.io/?page=39\n",
      "https://litecoinnews.io/?page=40\n",
      "https://litecoinnews.io/?page=41\n",
      "https://litecoinnews.io/?page=42\n",
      "https://litecoinnews.io/?page=43\n",
      "https://litecoinnews.io/?page=44\n",
      "https://litecoinnews.io/?page=45\n",
      "https://litecoinnews.io/?page=46\n",
      "https://litecoinnews.io/?page=47\n",
      "https://litecoinnews.io/?page=48\n",
      "https://litecoinnews.io/?page=49\n",
      "https://litecoinnews.io/?page=50\n",
      "https://litecoinnews.io/?page=51\n",
      "https://litecoinnews.io/?page=52\n",
      "https://litecoinnews.io/?page=53\n",
      "https://litecoinnews.io/?page=54\n",
      "https://litecoinnews.io/?page=55\n",
      "https://litecoinnews.io/?page=56\n",
      "https://litecoinnews.io/?page=57\n",
      "https://litecoinnews.io/?page=58\n",
      "https://litecoinnews.io/?page=59\n",
      "https://litecoinnews.io/?page=60\n",
      "https://litecoinnews.io/?page=61\n",
      "https://litecoinnews.io/?page=62\n",
      "https://litecoinnews.io/?page=63\n",
      "https://litecoinnews.io/?page=64\n",
      "https://litecoinnews.io/?page=65\n",
      "https://litecoinnews.io/?page=66\n",
      "https://litecoinnews.io/?page=67\n",
      "https://litecoinnews.io/?page=68\n",
      "https://litecoinnews.io/?page=69\n",
      "https://litecoinnews.io/?page=70\n",
      "https://litecoinnews.io/?page=71\n",
      "https://litecoinnews.io/?page=72\n",
      "https://litecoinnews.io/?page=73\n",
      "https://litecoinnews.io/?page=74\n",
      "https://litecoinnews.io/?page=75\n",
      "\n",
      "Done scraping all data and stored in list 'all_data_raw'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"Scraping litecoin news from litecoinnews.io\")\n",
    "    print(\"*******************************************\\n\")\n",
    "    litecoin_raw = get_litecoinNews()\n",
    "    \n",
    "    all_data_raw = []\n",
    "    all_data_raw.extend(list(litecoin_raw))\n",
    "    \n",
    "    print(\"\\nDone scraping all data and stored in list 'all_data_raw'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying the headlines as positive, negative and neutral and printing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral headlines:  35\n",
      "Somewhat negative headlines:  9\n",
      "Very negative headlines:  0\n",
      "Somewhat positive headlines:  11\n",
      "Very positive headlines:  2\n",
      "Total number of headlines:  57\n"
     ]
    }
   ],
   "source": [
    "#Filter out headlines without these keywords\n",
    "\n",
    "keywords = ['litecoin', 'cryptocurrency','cryptocurrencies', 'crypto', 'blockchain', 'blockchains']\n",
    "\n",
    "#Get headlines and dates as individual lists\n",
    "\n",
    "headlines = get_headlines(list(filter(lambda x: any(word in x[0] for word in keywords), all_data_raw)),0)\n",
    "dates = get_date(list(filter(lambda x: any(word in x[0] for word in keywords), all_data_raw)),1)\n",
    "sid = SentimentIntensityAnalyzer()       \n",
    "compound = []\n",
    "\n",
    "for head in headlines:\n",
    "    \n",
    "    head_lower = head.lower()\n",
    "    ss = sid.polarity_scores(head_lower)\n",
    "    compound.append(ss['compound'])\n",
    "    \n",
    "#Counting the number of headlines in each sentiment class\n",
    "\n",
    "neutral = []\n",
    "somewhat_negative = []\n",
    "somewhat_positive = []\n",
    "very_negative = []\n",
    "very_positive = []\n",
    "\n",
    "for index, score in enumerate(compound):\n",
    "    \n",
    "    if score > -0.20 and score < 0.20:\n",
    "        neutral.append(score)\n",
    "        \n",
    "    elif score > -0.60 and score < -0.20:\n",
    "        somewhat_negative.append(score)\n",
    "        \n",
    "    elif score > 0.20 and score < 0.60:\n",
    "        somewhat_positive.append(score)\n",
    "        \n",
    "    elif score <= -0.60:\n",
    "        very_negative.append(score)\n",
    "        \n",
    "    else:\n",
    "        very_positive.append(score)\n",
    "\n",
    "print('Neutral headlines: ', len(neutral))\n",
    "print('Somewhat negative headlines: ', len(somewhat_negative))\n",
    "print(\"Very negative headlines: \", len(very_negative))\n",
    "print('Somewhat positive headlines: ', len(somewhat_positive))\n",
    "print(\"Very positive headlines: \", len(very_positive))\n",
    "print('Total number of headlines: ', len(compound))\n",
    "\n",
    "data_with_sentiment = list(zip(dates, headlines, compound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing, lemmatizing and removing frequent keywords to improve clustering performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize and lemmatize headlines\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    \n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    \n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    \n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    \n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def lemmatize(document):\n",
    "    \n",
    "    pattern=r'[a-zA-Z]+[a-zA-Z\\-]+[a-zA-Z]'      \n",
    "    tokens=nltk.regexp_tokenize(document, pattern)\n",
    "    tagged_tokens=nltk.pos_tag(tokens)\n",
    "    wordnet_lemmatizer=WordNetLemmatizer()\n",
    "    \n",
    "    le_words=[wordnet_lemmatizer.lemmatize(word, get_wordnet_pos(tag)) \\\n",
    "              for (word, tag) in tagged_tokens \\\n",
    "              if word not in stop_words and \\\n",
    "              word not in string.punctuation]\n",
    "    \n",
    "    le_words = list(set(le_words))\n",
    "    return le_words\n",
    "\n",
    "lem_headlines = []\n",
    "\n",
    "for headline in headlines:\n",
    "    \n",
    "    lem_headline = lemmatize(headline)\n",
    "    lem_headlines.append(lem_headline)\n",
    "\n",
    "lem_data_with_sentiment = list(zip(dates, headlines, lem_headlines, compound))\n",
    "\n",
    "#Function to remove frequent keywords from lemmatized headline tokens to improve clustering performance\n",
    "\n",
    "def remove_keywords(data):\n",
    "    for row in data:\n",
    "        lem_list = row[2]\n",
    "        for word in lem_list:\n",
    "            if word in keywords:\n",
    "                lem_list.remove(word)\n",
    "        \n",
    "    return data\n",
    "\n",
    "#Preparing data for LDA clustering\n",
    "\n",
    "data_for_clustering = remove_keywords(lem_data_with_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LDA clustering model for 5 clusters and storing the data in \"Cleaned_data_final_5clusters_ltc.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.033*\"market\" + 0.025*\"price\" + 0.025*\"Litecoin\" + 0.017*\"founder\" + 0.017*\"The\"'), (1, '0.057*\"Litecoin\" + 0.050*\"LTC\" + 0.047*\"USD\" + 0.047*\"price\" + 0.030*\"market\"'), (2, '0.062*\"Litecoin\" + 0.032*\"LTC\" + 0.022*\"The\" + 0.022*\"FOMC\" + 0.022*\"wait\"'), (3, '0.024*\"LTC\" + 0.020*\"Ripple\" + 0.020*\"exchange\" + 0.020*\"trading\" + 0.020*\"payment\"'), (4, '0.040*\"Litecoin\" + 0.017*\"add\" + 0.017*\"say\" + 0.017*\"sell\" + 0.017*\"week\"')]\n",
      "\n",
      "All data cleaned, clustered and written to csv file 'Cleaned_data_final_5clusters_ltc.csv'\n"
     ]
    }
   ],
   "source": [
    "#Building LDA Model for 5 clusters\n",
    "\n",
    "headline = get_headlines(data_for_clustering,2)\n",
    "dictionary = corpora.Dictionary(headline)\n",
    "corpus = [dictionary.doc2bow(row) for row in headline]\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word = dictionary, passes=20)\n",
    "print(ldamodel.print_topics(num_topics=5, num_words=5))\n",
    "\n",
    "#Classifying clusters and mapping them to headlines\n",
    "\n",
    "cluster = ldamodel[corpus]\n",
    "clusters = []\n",
    "list_ = []\n",
    "\n",
    "for t in cluster:\n",
    "    list_ = (list(zip(*t))[1])\n",
    "    clusters.append(list_.index(max(list_)))\n",
    "\n",
    "date, headline, tokenized_headline, sentiment = zip(*data_for_clustering)\n",
    "clean_data_5clusters = list(zip(date, headline, tokenized_headline, sentiment, clusters))\n",
    "\n",
    "#Displaying the headlines within each cluster\n",
    "\n",
    "filter(lambda x: x[1] == 0, clean_data_5clusters)\n",
    "filter(lambda x: x[1] == 1, clean_data_5clusters)\n",
    "filter(lambda x: x[1] == 2, clean_data_5clusters)\n",
    "filter(lambda x: x[1] == 3, clean_data_5clusters)\n",
    "filter(lambda x: x[1] == 4, clean_data_5clusters)\n",
    "\n",
    "#Writing cleaned data to csv for 5 clusters\n",
    "\n",
    "data_frame_5clusters = pd.DataFrame.from_records(clean_data_5clusters, columns = [\"Date\", \"Headline\", \"Tokenized Headline\", \"Sentiment\", \"Cluster\"])\n",
    "data_frame_5clusters.to_csv(\"Cleaned_data_final_5clusters_ltc.csv\", encoding='utf-8')\n",
    "print(\"\\nAll data cleaned, clustered and written to csv file 'Cleaned_data_final_5clusters_ltc.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LDA clustering model for 4 clusters and storing the data in \"Cleaned_data_final_4clusters_ltc.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.052*\"Litecoin\" + 0.024*\"market\" + 0.023*\"LTC\" + 0.020*\"founder\" + 0.016*\"The\"'), (1, '0.022*\"Litecoin\" + 0.015*\"say\" + 0.015*\"surge\" + 0.015*\"payment\" + 0.015*\"bitcoin\"'), (2, '0.028*\"bank\" + 0.028*\"LTC\" + 0.028*\"Litecoin\" + 0.020*\"new\" + 0.019*\"solution\"'), (3, '0.068*\"price\" + 0.056*\"USD\" + 0.051*\"Litecoin\" + 0.047*\"LTC\" + 0.027*\"market\"')]\n",
      "\n",
      "All data cleaned, clustered and written to csv file 'Cleaned_data_final_4clusters_ltc.csv'\n"
     ]
    }
   ],
   "source": [
    "#Building LDA Model for 4 clusters\n",
    " \n",
    "headline_list = get_headlines(data_for_clustering,2)\n",
    "dictionary = corpora.Dictionary(headline_list)\n",
    "corpus = [dictionary.doc2bow(row) for row in headline_list]\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=4, id2word = dictionary, passes=20)\n",
    "print(ldamodel.print_topics(num_topics=4, num_words=5))\n",
    "\n",
    "#Classifying clusters and mapping them to headlines\n",
    "\n",
    "cluster = ldamodel[corpus]\n",
    "clusters = []\n",
    "list_ = []\n",
    "\n",
    "for t in cluster:\n",
    "    list_ = (list(zip(*t))[1])\n",
    "    clusters.append(list_.index(max(list_)))\n",
    "\n",
    "date, headline, tokenized_headline, sentiment = zip(*data_for_clustering)\n",
    "clean_data_4clusters = list(zip(date, headline, tokenized_headline, sentiment, clusters))\n",
    "\n",
    "#Displaying the headlines within each cluster\n",
    "\n",
    "filter(lambda x: x[1] == 0, clean_data_4clusters)\n",
    "filter(lambda x: x[1] == 1, clean_data_4clusters)\n",
    "filter(lambda x: x[1] == 2, clean_data_4clusters)\n",
    "filter(lambda x: x[1] == 3, clean_data_4clusters)\n",
    "\n",
    "#Writing cleaned data to csv for 4 clusters\n",
    "\n",
    "data_frame_4clusters = pd.DataFrame.from_records(clean_data_4clusters, columns = [\"Date\", \"Headline\", \"Tokenized Headline\", \"Sentiment\", \"Cluster\"])\n",
    "data_frame_4clusters.to_csv(\"Cleaned_data_final_4clusters_ltc.csv\", encoding='utf-8')\n",
    "print(\"\\nAll data cleaned, clustered and written to csv file 'Cleaned_data_final_4clusters_ltc.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating pivot tables based on dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating pivot tables by date\n",
    "\n",
    "pivot_table_5 = pd.pivot_table(data_frame_5clusters, values = 'Sentiment', index = 'Date', columns = 'Cluster', aggfunc = np.sum, fill_value = 0)\n",
    "pivot_table_4 = pd.pivot_table(data_frame_4clusters, values = 'Sentiment', index = 'Date', columns = 'Cluster', aggfunc = np.sum, fill_value = 0)\n",
    "\n",
    "pivot_table_5 = pivot_table_5.rename_axis(None, axis =1).reset_index()\n",
    "pivot_table_4 = pivot_table_4.rename_axis(None, axis =1).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging pivot tables with litecoin pricing data stored in files \"Litecoin1Day.csv\" and \"Litecoin3Day.csv\". These files have data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge pivot tables with bitcoin pricing data\n",
    "\n",
    "with open(\"Litecoin1Day.csv\", \"r\") as f:\n",
    "    \n",
    "    reader=csv.reader(f, delimiter=',') \n",
    "    litecoin_1day = [row for row in reader]\n",
    "    del litecoin_1day[0]\n",
    "\n",
    "date_list = []\n",
    "\n",
    "for date in get_date(litecoin_1day,0):\n",
    "    date = datetime.strptime(date, '%m/%d/%Y')\n",
    "    date = datetime.strftime(date,'%b %d, %Y')\n",
    "    date_list.append(date)\n",
    "    \n",
    "date, dailyreturn = zip(*litecoin_1day)\n",
    "litecoin_1day = zip(date_list,dailyreturn)\n",
    "    \n",
    "litecoin_1day = pd.DataFrame.from_records(litecoin_1day, columns = [\"Date\",\"DailyReturn\"])\n",
    "litecoin_1day\n",
    "\n",
    "litecoin_1day_5clusters = litecoin_1day.merge(pivot_table_5, on = 'Date', how = 'left')\n",
    "litecoin_1day_5clusters = litecoin_1day_5clusters.dropna()\n",
    "\n",
    "litecoin_1day_4clusters = litecoin_1day.merge(pivot_table_4, on = 'Date', how = 'left')\n",
    "litecoin_1day_4clusters = litecoin_1day_4clusters.dropna()\n",
    "\n",
    "with open(\"Litecoin3Day.csv\", \"r\") as f:\n",
    "    \n",
    "    reader=csv.reader(f, delimiter=',') \n",
    "    litecoin_3day = [row for row in reader]\n",
    "    del litecoin_3day[0]\n",
    "\n",
    "date_list = []\n",
    "\n",
    "for date in get_date(litecoin_3day,0):\n",
    "    date = datetime.strptime(date, '%m/%d/%Y')\n",
    "    date = datetime.strftime(date,'%b %d, %Y')\n",
    "    date_list.append(date)\n",
    "    \n",
    "date, dailyreturn = zip(*litecoin_3day)\n",
    "litecoin_3day = zip(date_list,dailyreturn)\n",
    "    \n",
    "litecoin_3day = pd.DataFrame.from_records(litecoin_3day, columns = [\"Date\",\"DailyReturn\"])\n",
    "\n",
    "litecoin_3day_5clusters = litecoin_3day.merge(pivot_table_5, on = 'Date', how = 'left')\n",
    "litecoin_3day_5clusters = litecoin_3day_5clusters.dropna()\n",
    "\n",
    "litecoin_3day_4clusters = litecoin_3day.merge(pivot_table_4, on = 'Date', how = 'left')\n",
    "litecoin_3day_4clusters = litecoin_3day_4clusters.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            DailyReturn   R-squared:                       0.151\n",
      "Model:                            OLS   Adj. R-squared:                 -0.006\n",
      "Method:                 Least Squares   F-statistic:                    0.9599\n",
      "Date:                Thu, 26 Apr 2018   Prob (F-statistic):              0.459\n",
      "Time:                        23:51:59   Log-Likelihood:                -130.02\n",
      "No. Observations:                  33   AIC:                             272.0\n",
      "Df Residuals:                      27   BIC:                             281.0\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          4.2502      2.642      1.609      0.119      -1.171       9.671\n",
      "0             10.9684     15.161      0.723      0.476     -20.140      42.077\n",
      "1             -3.3637     10.718     -0.314      0.756     -25.356      18.629\n",
      "2            -31.3856     19.193     -1.635      0.114     -70.767       7.995\n",
      "3            -14.1376     54.578     -0.259      0.798    -126.122      97.847\n",
      "4             26.0469     21.289      1.224      0.232     -17.634      69.728\n",
      "==============================================================================\n",
      "Omnibus:                       19.230   Durbin-Watson:                   1.411\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               24.629\n",
      "Skew:                           1.626   Prob(JB):                     4.49e-06\n",
      "Kurtosis:                       5.708   Cond. No.                         23.1\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "******************************************************************************************************\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            DailyReturn   R-squared:                       0.044\n",
      "Model:                            OLS   Adj. R-squared:                 -0.093\n",
      "Method:                 Least Squares   F-statistic:                    0.3214\n",
      "Date:                Thu, 26 Apr 2018   Prob (F-statistic):              0.861\n",
      "Time:                        23:51:59   Log-Likelihood:                -131.98\n",
      "No. Observations:                  33   AIC:                             274.0\n",
      "Df Residuals:                      28   BIC:                             281.4\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.7840      2.903      0.615      0.544      -4.162       7.730\n",
      "0             -6.4074     14.379     -0.446      0.659     -35.861      23.046\n",
      "1             -0.5121     16.941     -0.030      0.976     -35.214      34.190\n",
      "2              0.1221     11.482      0.011      0.992     -23.397      23.641\n",
      "3             41.7057     38.489      1.084      0.288     -37.135     120.546\n",
      "==============================================================================\n",
      "Omnibus:                       27.737   Durbin-Watson:                   1.356\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               49.645\n",
      "Skew:                           2.106   Prob(JB):                     1.66e-11\n",
      "Kurtosis:                       7.286   Cond. No.                         15.5\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "******************************************************************************************************\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            DailyReturn   R-squared:                       0.114\n",
      "Model:                            OLS   Adj. R-squared:                 -0.050\n",
      "Method:                 Least Squares   F-statistic:                    0.6968\n",
      "Date:                Thu, 26 Apr 2018   Prob (F-statistic):              0.630\n",
      "Time:                        23:51:59   Log-Likelihood:                -157.39\n",
      "No. Observations:                  33   AIC:                             326.8\n",
      "Df Residuals:                      27   BIC:                             335.8\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          9.5291      6.055      1.574      0.127      -2.894      21.952\n",
      "0             14.4062     34.745      0.415      0.682     -56.884      85.697\n",
      "1              8.7527     24.563      0.356      0.724     -41.647      59.152\n",
      "2            -63.5454     43.985     -1.445      0.160    -153.794      26.703\n",
      "3             94.5598    125.075      0.756      0.456    -162.074     351.193\n",
      "4             20.5726     48.787      0.422      0.677     -79.530     120.676\n",
      "==============================================================================\n",
      "Omnibus:                       18.766   Durbin-Watson:                   0.770\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               23.031\n",
      "Skew:                           1.643   Prob(JB):                     9.97e-06\n",
      "Kurtosis:                       5.438   Cond. No.                         23.1\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "******************************************************************************************************\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            DailyReturn   R-squared:                       0.146\n",
      "Model:                            OLS   Adj. R-squared:                  0.024\n",
      "Method:                 Least Squares   F-statistic:                     1.200\n",
      "Date:                Thu, 26 Apr 2018   Prob (F-statistic):              0.333\n",
      "Time:                        23:51:59   Log-Likelihood:                -156.78\n",
      "No. Observations:                  33   AIC:                             323.6\n",
      "Df Residuals:                      28   BIC:                             331.0\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          6.3885      6.155      1.038      0.308      -6.219      18.996\n",
      "0            -26.7128     30.486     -0.876      0.388     -89.161      35.735\n",
      "1             64.9259     35.918      1.808      0.081      -8.649     138.501\n",
      "2             -7.8601     24.343     -0.323      0.749     -57.725      42.005\n",
      "3             54.9615     81.603      0.674      0.506    -112.195     222.118\n",
      "==============================================================================\n",
      "Omnibus:                       19.879   Durbin-Watson:                   0.707\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               26.195\n",
      "Skew:                           1.659   Prob(JB):                     2.05e-06\n",
      "Kurtosis:                       5.836   Cond. No.                         15.5\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "******************************************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1 day 5 clusters\n",
    "\n",
    "x_train = litecoin_1day_5clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = litecoin_1day_5clusters['DailyReturn']\n",
    "\n",
    "model_1_5 = sm.OLS(y_train.astype(float), x_train.astype(float))\n",
    "results_1_5 = model_1_5.fit()\n",
    "print(results_1_5.summary())\n",
    "print('\\n******************************************************************************************************\\n')\n",
    "\n",
    "#1 day 4 clusters\n",
    "\n",
    "x_train = litecoin_1day_4clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = litecoin_1day_4clusters['DailyReturn']\n",
    "\n",
    "model_1_4 = sm.OLS(y_train.astype(float), x_train.astype(float))\n",
    "results_1_4 = model_1_4.fit()\n",
    "print(results_1_4.summary())\n",
    "print('\\n******************************************************************************************************\\n')\n",
    "\n",
    "#3 day 5 clusters\n",
    "\n",
    "x_train = litecoin_3day_5clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = litecoin_3day_5clusters['DailyReturn']\n",
    "\n",
    "model_3_5 = sm.OLS(y_train.astype(float), x_train.astype(float))\n",
    "results_3_5 = model_3_5.fit()\n",
    "print(results_3_5.summary())\n",
    "print('\\n******************************************************************************************************\\n')\n",
    "pred = results_3_5.predict(x_train.astype(float))\n",
    "\n",
    "#3 day 4 clusters\n",
    "\n",
    "x_train = litecoin_3day_4clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = litecoin_3day_4clusters['DailyReturn']\n",
    "\n",
    "model_3_4 = sm.OLS(y_train.astype(float), x_train.astype(float))\n",
    "results_3_4 = model_3_4.fit()\n",
    "print(results_3_4.summary())\n",
    "print('\\n******************************************************************************************************\\n')\n",
    "\n",
    "litecoin_1day_5clusters_binary = []\n",
    "\n",
    "for value in litecoin_1day_5clusters['DailyReturn']:\n",
    "    if float(value) > 0:\n",
    "        value_binary = 1\n",
    "    else:\n",
    "        value_binary = 0\n",
    "        \n",
    "    litecoin_1day_5clusters_binary.append(value_binary)\n",
    "    \n",
    "litecoin_1day_4clusters_binary = []\n",
    "\n",
    "for value in litecoin_1day_4clusters['DailyReturn']:\n",
    "    if float(value) > 0:\n",
    "        value_binary = 1\n",
    "    else:\n",
    "        value_binary = 0\n",
    "        \n",
    "    litecoin_1day_4clusters_binary.append(value_binary)\n",
    "    \n",
    "litecoin_3day_5clusters_binary = []\n",
    "\n",
    "for value in litecoin_3day_5clusters['DailyReturn']:\n",
    "    if float(value) > 0:\n",
    "        value_binary = 1\n",
    "    else:\n",
    "        value_binary = 0\n",
    "        \n",
    "    litecoin_3day_5clusters_binary.append(value_binary)\n",
    "    \n",
    "litecoin_3day_4clusters_binary = []\n",
    "\n",
    "for value in litecoin_3day_4clusters['DailyReturn']:\n",
    "    if float(value) > 0:\n",
    "        value_binary = 1\n",
    "    else:\n",
    "        value_binary = 0\n",
    "        \n",
    "    litecoin_3day_4clusters_binary.append(value_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.59      0.75        32\n",
      "          1       0.07      1.00      0.13         1\n",
      "\n",
      "avg / total       0.97      0.61      0.73        33\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.59      0.75        32\n",
      "          1       0.07      1.00      0.13         1\n",
      "\n",
      "avg / total       0.97      0.61      0.73        33\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.54      0.65        26\n",
      "          1       0.25      0.57      0.35         7\n",
      "\n",
      "avg / total       0.70      0.55      0.59        33\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.53      0.68        30\n",
      "          1       0.12      0.67      0.21         3\n",
      "\n",
      "avg / total       0.87      0.55      0.64        33\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LogReg = LogisticRegression()\n",
    "\n",
    "#1 day 5 clusters\n",
    "\n",
    "x_train = litecoin_1day_5clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = litecoin_1day_5clusters_binary\n",
    "\n",
    "model_1_5_log = LogReg.fit(x_train.astype(float),y_train)\n",
    "y_pred = model_1_5_log.predict(x_train.astype(float))\n",
    "print(classification_report(y_pred, y_train))\n",
    "\n",
    "#1 day 4 clusters\n",
    "\n",
    "x_train = litecoin_1day_4clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = litecoin_1day_4clusters_binary\n",
    "\n",
    "model_1_4_log = LogReg.fit(x_train.astype(float),y_train)\n",
    "y_pred = model_1_4_log.predict(x_train.astype(float))\n",
    "print(classification_report(y_pred, y_train))\n",
    "\n",
    "#3 day 5 clusters\n",
    "\n",
    "x_train = litecoin_3day_5clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = litecoin_3day_5clusters_binary\n",
    "\n",
    "model_3_5_log = LogReg.fit(x_train.astype(float),y_train)\n",
    "y_pred = model_3_5_log.predict(x_train.astype(float))\n",
    "print(classification_report(y_pred, y_train))\n",
    "\n",
    "#3 day 4 clusters\n",
    "\n",
    "x_train = litecoin_3day_4clusters.iloc[:,2:]\n",
    "x_train = sm.add_constant(x_train)\n",
    "y_train = litecoin_3day_4clusters_binary\n",
    "\n",
    "model_3_4_log = LogReg.fit(x_train.astype(float),y_train)\n",
    "y_pred = model_3_4_log.predict(x_train.astype(float))\n",
    "print(classification_report(y_pred, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputing the actual vs predicted returns in \"Actual_Predicted_1day_5clust_ltc.csv\", \"Actual_Predicted_1day_4clust_ltc.csv\", \"Actual_Predicted_3day_5clust_ltc.csv\" and \"Actual_Predicted_3day_4clust_ltc.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actual vs Predicted value comparison\n",
    "#1 day 5 clusters\n",
    "\n",
    "actual = litecoin_1day_5clusters['DailyReturn'].tolist()\n",
    "predicted = pred.tolist()\n",
    "actual_predicted = list(zip(actual, predicted))\n",
    "\n",
    "actual_predicted = pd.DataFrame.from_records(actual_predicted, columns = [\"Actual Return\", \"Predicted Return\"])\n",
    "actual_predicted.to_csv(\"Actual_Predicted_1day_5clust_ltc.csv\", encoding='utf-8')\n",
    "\n",
    "#1 day 4 clusters\n",
    "\n",
    "actual = litecoin_1day_4clusters['DailyReturn'].tolist()\n",
    "predicted = pred.tolist()\n",
    "actual_predicted = list(zip(actual, predicted))\n",
    "\n",
    "actual_predicted = pd.DataFrame.from_records(actual_predicted, columns = [\"Actual Return\", \"Predicted Return\"])\n",
    "actual_predicted.to_csv(\"Actual_Predicted_1day_4clust_ltc.csv\", encoding='utf-8')\n",
    "\n",
    "#3 day 5 clusters\n",
    "\n",
    "actual = litecoin_3day_5clusters['DailyReturn'].tolist()\n",
    "predicted = pred.tolist()\n",
    "actual_predicted = list(zip(actual, predicted))\n",
    "\n",
    "actual_predicted = pd.DataFrame.from_records(actual_predicted, columns = [\"Actual Return\", \"Predicted Return\"])\n",
    "actual_predicted.to_csv(\"Actual_Predicted_3day_5clust_ltc.csv\", encoding='utf-8')\n",
    "\n",
    "#3 day 4 clusters\n",
    "\n",
    "actual = litecoin_3day_4clusters['DailyReturn'].tolist()\n",
    "predicted = pred.tolist()\n",
    "actual_predicted = list(zip(actual, predicted))\n",
    "\n",
    "actual_predicted = pd.DataFrame.from_records(actual_predicted, columns = [\"Actual Return\", \"Predicted Return\"])\n",
    "actual_predicted.to_csv(\"Actual_Predicted_3day_4clust_ltc.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
